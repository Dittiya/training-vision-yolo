{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869e6593",
   "metadata": {},
   "source": [
    "# Dir Structure\n",
    "```\n",
    "<your-repo>\n",
    "|_ dataset\n",
    "    |_ dataset.yaml\n",
    "|_ yolov5*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981507a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda is used\n",
    "import torch\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a18bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8059b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset.yaml file\n",
    "if not path.exists('datasets'):\n",
    "    os.mkdir('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4c7086",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocess raw images\n",
    "def load_image(path):\n",
    "    size = (1280, 720)\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return img.resize(size)\n",
    "\n",
    "# crop image for standard 1280x720\n",
    "def crop_face(img):\n",
    "    size = img.size\n",
    "    center = img.size[0]*0.5\n",
    "    w = size[1]*0.2\n",
    "    \n",
    "    left = center-w\n",
    "    right = center+w\n",
    "    top = 75\n",
    "    bottom = 500\n",
    "    \n",
    "    img = img.crop((left,top,right,bottom))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess():\n",
    "    DIR = './datasets/training/raw'\n",
    "    SAVE = './datasets/training/images'\n",
    "    for file in os.listdir(DIR):\n",
    "        img = load_image(path.join(DIR, file))\n",
    "        img = crop_face(img)\n",
    "        img.save(f\"{SAVE}/{file.split('.')[0]}_body.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff0693c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images without label\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check image health (missing labels, missing images, etc.)\n",
    "def check_labels():\n",
    "    DIR = 'datasets/training_small'\n",
    "    results = []\n",
    "    for file in os.listdir(path.join(DIR, 'labels')):\n",
    "        name = file.split('.')[0] + '.jpg'\n",
    "        for root, dirs, files in os.walk(path.join(DIR, 'images')):\n",
    "            if name not in files:\n",
    "                results.append(name)\n",
    "                \n",
    "    if len(results) == 0:\n",
    "        return 'labels are healthy'\n",
    "    return results\n",
    "\n",
    "def check_images():\n",
    "    DIR = 'datasets/training_small'\n",
    "    results = []\n",
    "    for file in os.listdir(path.join(DIR, 'images')):\n",
    "        name = file.split('.')[0] + '.txt'\n",
    "        for root, dirs, files in os.walk(path.join(DIR, 'labels')):\n",
    "            if name not in files:\n",
    "                results.append(file)\n",
    "    return results\n",
    "\n",
    "def move_files(files):\n",
    "    source = 'datasets/training_small/images'\n",
    "    dest = 'datasets/training_small'\n",
    "    for file in files:\n",
    "        full_path = path.join(source, file)\n",
    "        shutil.move(full_path, dest)\n",
    "\n",
    "print('Images without label')\n",
    "x = check_images()\n",
    "print(len(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d80aa",
   "metadata": {},
   "source": [
    "# Template\n",
    "`python yolov5/train.py --img 640 --batch 2 --epochs 1 --data datasets/training/ak.yaml --weights yolov5/yolov5s.pt --hyp hyp.custom.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9913ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model (remove mosaic)\n",
    "python yolov5/train.py --img 640 --batch 2 --epochs 2 --data datasets/training/ak.yaml --weights yolov5/yolov5s.pt --hyp hyp.custom.yaml --rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on smaller dataset (experimental)\n",
    "python yolov5/train.py --img 640 --batch 2 --epochs 200 --data datasets/training_small/ak.yaml --weights yolov5/yolov5m.pt --hyp hyp.custom.yaml --rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69c19b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference images need to have dimensions of 640x640 \n",
    "def pre_inference():\n",
    "    DIR = './datasets/inference/raw'\n",
    "    SAVE = './datasets/inference/images'\n",
    "    \n",
    "    left = (0, 0, 640, 640)\n",
    "    right = (640, 0, 1280, 640)\n",
    "    crops = {'left': left, 'right': right}\n",
    "    \n",
    "    for file in os.listdir(DIR):\n",
    "        img = load_image(path.join(DIR, file))\n",
    "        \n",
    "        for k, v in crops.items():\n",
    "            proc = img.crop(v)\n",
    "            proc.save(f\"{SAVE}/{file.split('.')[0]}_{k}.jpg\")\n",
    "        \n",
    "pre_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec9ff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\aditi/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-7-12 Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Loading yolov5\\runs\\train\\medium_baseline_200\\weights\\best.onnx for ONNX Runtime inference...\n",
      "C:\\Users\\aditi\\miniconda3\\envs\\yolo\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_path = 'yolov5/runs/train/medium_baseline_200/weights/best.onnx'\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86644b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 640x640 2 hibiscuss\n",
      "Speed: 8.1ms pre-process, 159.9ms inference, 3.8ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 99.17775, 223.11775, 203.21819, 362.58972,   0.92617,   7.00000],\n",
       "         [302.93692, 221.52496, 401.61923, 367.02466,   0.90101,   7.00000]], device='cuda:0')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conf = 0.5\n",
    "img = 'datasets/inference/images/oVcEpND_right.jpg'\n",
    "result = model(img, size=640)\n",
    "\n",
    "result.print()\n",
    "\n",
    "result.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bc809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/medium_baseline_200/weights/best.pt'], source=datasets/inference/images, data=yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5\\runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.1-289-g526e650 Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 290 layers, 20913549 parameters, 0 gradients, 48.1 GFLOPs\n",
      "image 1/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\11629306773920_left.jpg: 640x640 Done. (0.018s)\n",
      "image 2/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\11629306773920_right.jpg: 640x640 1 midnight, 2 fangs, Done. (0.022s)\n",
      "image 3/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165331840_left.jpg: 640x640 1 cardigan, 1 beagle, 1 melantha, Done. (0.018s)\n",
      "image 4/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165331840_right.jpg: 640x640 1 popukar, Done. (0.017s)\n",
      "image 5/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165341389_left.jpg: 640x640 1 spot, 1 vanilla, 1 fang, 1 melantha, Done. (0.020s)\n",
      "image 6/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165341389_right.jpg: 640x640 1 lava, 1 fang, Done. (0.018s)\n",
      "image 7/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165612798_left.jpg: 640x640 1 melantha, 1 catapult, Done. (0.019s)\n",
      "image 8/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165612798_right.jpg: 640x640 1 cardigan, 1 plume, Done. (0.016s)\n",
      "image 9/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651166047758_left.jpg: 640x640 2 vanillas, Done. (0.016s)\n",
      "image 10/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651166047758_right.jpg: 640x640 1 orchid, Done. (0.016s)\n",
      "image 11/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1652318725751_right.jpg: 640x640 Done. (0.015s)\n",
      "image 12/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\2y0hFl6_left.jpg: 640x640 1 popukar, Done. (0.016s)\n",
      "image 13/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\2y0hFl6_right.jpg: 640x640 1 spot, 1 vanilla, 1 catapult, Done. (0.016s)\n",
      "image 14/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\D0cHa3D_left.jpg: 640x640 1 popukar, 1 vanilla, Done. (0.016s)\n",
      "image 15/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\D0cHa3D_right.jpg: 640x640 1 fang, Done. (0.016s)\n",
      "image 16/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\_left.jpg: 640x640 1 hibiscus, 1 fang, Done. (0.016s)\n",
      "image 17/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\_right.jpg: 640x640 1 beagle, Done. (0.016s)\n",
      "image 18/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\arknights-discount-pull_left.jpg: 640x640 1 cardigan, Done. (0.015s)\n",
      "image 19/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\arknights-discount-pull_right.jpg: 640x640 1 kroos, 1 melantha, Done. (0.016s)\n",
      "image 20/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\oVcEpND_left.jpg: 640x640 1 plume, 1 orchid, 1 catapult, Done. (0.015s)\n",
      "image 21/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\oVcEpND_right.jpg: 640x640 2 hibiscuss, Done. (0.016s)\n",
      "image 22/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_1_left.jpg: 640x640 1 fang, Done. (0.015s)\n",
      "image 23/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_1_right.jpg: 640x640 1 midnight, 1 orchid, 1 ansel, Done. (0.016s)\n",
      "image 24/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_2_left.jpg: 640x640 1 melantha, Done. (0.017s)\n",
      "image 25/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_2_right.jpg: 640x640 1 spot, 1 cardigan, 1 steward, Done. (0.016s)\n",
      "image 26/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_3_left.jpg: 640x640 1 lava, 2 cardigans, 1 plume, 1 orchid, Done. (0.016s)\n",
      "image 27/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_3_right.jpg: 640x640 1 lava, 1 hibiscus, Done. (0.016s)\n",
      "image 28/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\swtSedm_left.jpg: 640x640 1 hibiscus, 1 fang, Done. (0.016s)\n",
      "image 29/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\swtSedm_right.jpg: 640x640 1 beagle, Done. (0.015s)\n",
      "Speed: 0.8ms pre-process, 16.5ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5\\runs\\detect\\exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/detect.py \\\n",
    "            --weights yolov5/runs/train/medium_baseline_200/weights/best.pt \\\n",
    "            --img 640 \\\n",
    "            --conf 0.5 \\\n",
    "            --source datasets/inference/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82873c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
