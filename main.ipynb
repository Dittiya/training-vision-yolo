{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869e6593",
   "metadata": {},
   "source": [
    "# Dir Structure\n",
    "```\n",
    "<your-repo>\n",
    "|_ dataset\n",
    "    |_ dataset.yaml\n",
    "|_ yolov5*\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981507a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aditi\\miniconda3\\envs\\yolo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda is used\n",
    "import torch\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a18bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8059b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset.yaml file\n",
    "if not path.exists('datasets'):\n",
    "    os.mkdir('datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4c7086",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocess raw images\n",
    "def load_image(path):\n",
    "    size = (1280, 720)\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return img.resize(size)\n",
    "\n",
    "# crop image for standard 1280x720\n",
    "def crop_face(img):\n",
    "    size = img.size\n",
    "    center = img.size[0]*0.5\n",
    "    w = size[1]*0.2\n",
    "    \n",
    "    left = center-w\n",
    "    right = center+w\n",
    "    top = 75\n",
    "    bottom = 500\n",
    "    \n",
    "    img = img.crop((left,top,right,bottom))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess():\n",
    "    DIR = './datasets/training/raw'\n",
    "    SAVE = './datasets/training/images'\n",
    "    for file in os.listdir(DIR):\n",
    "        img = load_image(path.join(DIR, file))\n",
    "        img = crop_face(img)\n",
    "        img.save(f\"{SAVE}/{file.split('.')[0]}_body.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0693c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images without label\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['background_1.jpg',\n",
       " 'background_2.jpg',\n",
       " 'background_3.jpg',\n",
       " 'background_4.jpg',\n",
       " 'background_5.jpg',\n",
       " 'background_6.jpg',\n",
       " 'background_7.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check image health (missing labels, missing images, etc.)\n",
    "def check_labels():\n",
    "    DIR = 'datasets/training_small'\n",
    "    results = []\n",
    "    for file in os.listdir(path.join(DIR, 'labels')):\n",
    "        name = file.split('.')[0] + '.jpg'\n",
    "        for root, dirs, files in os.walk(path.join(DIR, 'images')):\n",
    "            if name not in files:\n",
    "                results.append(name)\n",
    "                \n",
    "    if len(results) == 0:\n",
    "        return 'labels are healthy'\n",
    "    return results\n",
    "\n",
    "def check_images():\n",
    "    DIR = 'datasets/training_small'\n",
    "    results = []\n",
    "    for file in os.listdir(path.join(DIR, 'images')):\n",
    "        name = file.split('.')[0] + '.txt'\n",
    "        for root, dirs, files in os.walk(path.join(DIR, 'labels')):\n",
    "            if name not in files:\n",
    "                results.append(file)\n",
    "    return results\n",
    "\n",
    "def move_files(files):\n",
    "    source = 'datasets/training_small/images'\n",
    "    dest = 'datasets/training_small'\n",
    "    for file in files:\n",
    "        full_path = path.join(source, file)\n",
    "        shutil.move(full_path, dest)\n",
    "\n",
    "print('Images without label')\n",
    "x = check_images()\n",
    "print(len(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d80aa",
   "metadata": {},
   "source": [
    "# Template\n",
    "`python yolov5/train.py --img 640 --batch 2 --epochs 1 --data datasets/training/ak.yaml --weights yolov5/yolov5s.pt --hyp hyp.custom.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9913ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model (remove mosaic)\n",
    "python yolov5/train.py --img 640 --batch 2 --epochs 2 --data datasets/training/ak.yaml --weights yolov5/yolov5s.pt --hyp hyp.custom.yaml --rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0693bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "img_size = 640\n",
    "batch_size = 2\n",
    "epochs = 300\n",
    "data_loc = 'datasets/training_small/ak.yaml'\n",
    "weights = 'yolov5/yolov5s.pt'\n",
    "hyperparam = 'hyp.custom.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on smaller dataset (experimental)\n",
    "\n",
    "# medium yolov5\n",
    "python yolov5/train.py --img 640 --batch 2 --epochs 200 --data datasets/training_small/ak.yaml --weights yolov5/yolov5m.pt --hyp hyp.custom.yaml --rect\n",
    "\n",
    "# small yolov5\n",
    "python yolov5/train.py --img 640 --batch 2 --epochs 300 --data datasets/training_small/ak.yaml --weights yolov5/yolov5s.pt --hyp hyp.custom.yaml --rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617935ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/train.py \\\n",
    "    --img {img_size} \\\n",
    "    --batch {batch_size} \\\n",
    "    --epochs {epochs} \\\n",
    "    --data {data_loc} \\\n",
    "    --weights {weights} \\\n",
    "    --hyp {hyperparam} \\\n",
    "    --rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69c19b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference images need to have dimensions of 640x640 \n",
    "def pre_inference():\n",
    "    DIR = './datasets/inference/raw'\n",
    "    SAVE = './datasets/inference/images'\n",
    "    \n",
    "    left = (0, 0, 640, 640)\n",
    "    right = (640, 0, 1280, 640)\n",
    "    crops = {'left': left, 'right': right}\n",
    "    \n",
    "    for file in os.listdir(DIR):\n",
    "        img = load_image(path.join(DIR, file))\n",
    "        \n",
    "        for k, v in crops.items():\n",
    "            proc = img.crop(v)\n",
    "            proc.save(f\"{SAVE}/{file.split('.')[0]}_{k}.jpg\")\n",
    "        \n",
    "pre_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec9ff4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aditi\\miniconda3\\envs\\yolo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\aditi/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-7-12 Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Loading yolov5\\runs\\train\\training_test\\weights\\test.onnx for ONNX Runtime inference...\n",
      "C:\\Users\\aditi\\miniconda3\\envs\\yolo\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_path = 'yolov5/runs/train/training_test/weights/test.onnx'\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86644b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 640x640 1 hibiscus, 4 lavas\n",
      "Speed: 9.0ms pre-process, 124.7ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381.912292</td>\n",
       "      <td>199.964035</td>\n",
       "      <td>467.395813</td>\n",
       "      <td>295.345886</td>\n",
       "      <td>0.445480</td>\n",
       "      <td>1</td>\n",
       "      <td>lava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258.156494</td>\n",
       "      <td>202.558975</td>\n",
       "      <td>350.251648</td>\n",
       "      <td>310.185486</td>\n",
       "      <td>0.237979</td>\n",
       "      <td>0</td>\n",
       "      <td>hibiscus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>270.278687</td>\n",
       "      <td>88.331223</td>\n",
       "      <td>589.577209</td>\n",
       "      <td>395.320007</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>1</td>\n",
       "      <td>lava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403.665009</td>\n",
       "      <td>183.343689</td>\n",
       "      <td>495.277557</td>\n",
       "      <td>310.580383</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>1</td>\n",
       "      <td>lava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359.010101</td>\n",
       "      <td>151.389618</td>\n",
       "      <td>459.982208</td>\n",
       "      <td>319.383850</td>\n",
       "      <td>0.021440</td>\n",
       "      <td>1</td>\n",
       "      <td>lava</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin        xmax        ymax  confidence  class      name\n",
       "0  381.912292  199.964035  467.395813  295.345886    0.445480      1      lava\n",
       "1  258.156494  202.558975  350.251648  310.185486    0.237979      0  hibiscus\n",
       "2  270.278687   88.331223  589.577209  395.320007    0.027036      1      lava\n",
       "3  403.665009  183.343689  495.277557  310.580383    0.022632      1      lava\n",
       "4  359.010101  151.389618  459.982208  319.383850    0.021440      1      lava"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conf = 0.02\n",
    "img = 'datasets/inference/images/pull_3_right.jpg'\n",
    "result = model(img)\n",
    "\n",
    "result.print()\n",
    "\n",
    "result.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0210f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.81912e+02, 1.99964e+02, 4.67396e+02, 2.95346e+02, 4.45480e-01, 1.00000e+00],\n",
       "        [2.58156e+02, 2.02559e+02, 3.50252e+02, 3.10185e+02, 2.37979e-01, 0.00000e+00],\n",
       "        [2.70279e+02, 8.83312e+01, 5.89577e+02, 3.95320e+02, 2.70360e-02, 1.00000e+00],\n",
       "        [4.03665e+02, 1.83344e+02, 4.95278e+02, 3.10580e+02, 2.26324e-02, 1.00000e+00],\n",
       "        [3.59010e+02, 1.51390e+02, 4.59982e+02, 3.19384e+02, 2.14396e-02, 1.00000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbec21e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method AutoShape.forward of AutoShape(\n",
       "  (model): DetectMultiBackend()\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a6e52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = [[[1,2,3,4], [1,2,3,4], [1,2,3,4]]]\n",
    "output = np.array(arr)\n",
    "out = output.flatten()\n",
    "output\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bc809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/runs/train/exp2/weights/best.pt'], source=datasets/inference/images, data=yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.75, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5\\runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.1-289-g526e650 Python-3.9.12 torch-1.11.0+cu113 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7053277 parameters, 0 gradients, 15.9 GFLOPs\n",
      "image 1/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\11629306773920_left.jpg: 640x640 Done. (0.129s)\n",
      "image 2/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\11629306773920_right.jpg: 640x640 1 midnight, 2 fangs, Done. (0.132s)\n",
      "image 3/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165331840_left.jpg: 640x640 1 cardigan, 1 beagle, 1 melantha, Done. (0.131s)\n",
      "image 4/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165331840_right.jpg: 640x640 1 popukar, Done. (0.129s)\n",
      "image 5/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165341389_left.jpg: 640x640 1 spot, 1 vanilla, 1 fang, 1 melantha, Done. (0.132s)\n",
      "image 6/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165341389_right.jpg: 640x640 1 lava, 1 fang, Done. (0.130s)\n",
      "image 7/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165612798_left.jpg: 640x640 1 melantha, 1 catapult, Done. (0.130s)\n",
      "image 8/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651165612798_right.jpg: 640x640 Done. (0.131s)\n",
      "image 9/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651166047758_left.jpg: 640x640 2 vanillas, Done. (0.132s)\n",
      "image 10/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1651166047758_right.jpg: 640x640 1 orchid, Done. (0.126s)\n",
      "image 11/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\1652318725751_right.jpg: 640x640 Done. (0.126s)\n",
      "image 12/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\2y0hFl6_left.jpg: 640x640 1 popukar, Done. (0.128s)\n",
      "image 13/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\2y0hFl6_right.jpg: 640x640 1 spot, 1 vanilla, 1 catapult, Done. (0.127s)\n",
      "image 14/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\D0cHa3D_left.jpg: 640x640 1 popukar, 1 vanilla, Done. (0.142s)\n",
      "image 15/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\D0cHa3D_right.jpg: 640x640 1 fang, Done. (0.130s)\n",
      "image 16/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\_left.jpg: 640x640 1 hibiscus, 1 fang, Done. (0.128s)\n",
      "image 17/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\_right.jpg: 640x640 1 beagle, Done. (0.129s)\n",
      "image 18/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\arknights-discount-pull_left.jpg: 640x640 1 cardigan, Done. (0.132s)\n",
      "image 19/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\arknights-discount-pull_right.jpg: 640x640 1 kroos, 1 melantha, Done. (0.127s)\n",
      "image 20/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\oVcEpND_left.jpg: 640x640 1 catapult, Done. (0.129s)\n",
      "image 21/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\oVcEpND_right.jpg: 640x640 2 hibiscuss, Done. (0.133s)\n",
      "image 22/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_1_left.jpg: 640x640 1 fang, Done. (0.133s)\n",
      "image 23/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_1_right.jpg: 640x640 1 midnight, 1 orchid, 1 ansel, Done. (0.139s)\n",
      "image 24/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_2_left.jpg: 640x640 1 melantha, Done. (0.143s)\n",
      "image 25/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_2_right.jpg: 640x640 1 spot, 1 cardigan, 1 steward, Done. (0.156s)\n",
      "image 26/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_3_left.jpg: 640x640 2 cardigans, 1 orchid, Done. (0.134s)\n",
      "image 27/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\pull_3_right.jpg: 640x640 1 lava, 1 hibiscus, Done. (0.137s)\n",
      "image 28/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\swtSedm_left.jpg: 640x640 1 hibiscus, 1 fang, Done. (0.140s)\n",
      "image 29/29 D:\\Repository\\tracking-vision-yolo\\datasets\\inference\\images\\swtSedm_right.jpg: 640x640 1 beagle, Done. (0.132s)\n",
      "Speed: 0.7ms pre-process, 132.7ms inference, 0.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5\\runs\\detect\\exp3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/detect.py \\\n",
    "            --weights yolov5/runs/train/exp2/weights/best.pt \\\n",
    "            --img 640 \\\n",
    "            --conf 0.75 \\\n",
    "            --source datasets/inference/images \\\n",
    "            --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82873c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=D:\\Repository\\tracking-vision-yolo\\yolov5\\data\\coco128.yaml, weights=['yolov5/runs/train/exp2/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5  v6.1-289-g526e650 Python-3.9.12 torch-1.11.0+cu113 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7053277 parameters, 0 gradients, 15.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5\\runs\\train\\exp2\\weights\\best.pt with output shape (1, 25200, 21) (13.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.11.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as yolov5\\runs\\train\\exp2\\weights\\best.onnx (27.3 MB)\n",
      "\n",
      "Export complete (2.10s)\n",
      "Results saved to \u001b[1mD:\\Repository\\tracking-vision-yolo\\yolov5\\runs\\train\\exp2\\weights\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5\\runs\\train\\exp2\\weights\\best.onnx \n",
      "Validate:        python val.py --weights yolov5\\runs\\train\\exp2\\weights\\best.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5\\runs\\train\\exp2\\weights\\best.onnx')\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python yolov5/export.py --weights yolov5/runs/train/exp2/weights/best.pt --include onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d4a0d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\": {\"name\": \"midnight\"}, \"1\": {\"name\": \"lava\"}, \"2\": {\"name\": \"spot\"}, \"3\": {\"name\": \"cardigan\"}, \"4\": {\"name\": \"beagle\"}, \"5\": {\"name\": \"steward\"}, \"6\": {\"name\": \"plume\"}, \"7\": {\"name\": \"hibiscus\"}, \"8\": {\"name\": \"orchid\"}, \"9\": {\"name\": \"popukar\"}, \"10\": {\"name\": \"ansel\"}, \"11\": {\"name\": \"vanilla\"}, \"12\": {\"name\": \"fang\"}, \"13\": {\"name\": \"kroos\"}, \"14\": {\"name\": \"melantha\"}, \"15\": {\"name\": \"catapult\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "# converts classes.txt to JSON\n",
    "def create_dict():\n",
    "    FILE_PATH = \"./datasets/training_small/labels/classes.txt\"\n",
    "    f = open(FILE_PATH, \"r\")\n",
    "    classes_dict = {}\n",
    "    for i, name in enumerate(f.readlines()):\n",
    "        class_obj = {\"name\": name.strip()}\n",
    "        classes_dict[i] = class_obj\n",
    "        \n",
    "    return classes_dict\n",
    "\n",
    "x = json.dumps(create_dict())\n",
    "f = open(\"classes.json\", \"w\")\n",
    "f.write(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b32150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
